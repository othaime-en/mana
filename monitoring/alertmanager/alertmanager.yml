# AlertManager Configuration for Mana Self-Healing CI/CD
# Alert routing, grouping, and notification management

global:
  # Global timeout for alert resolution
  resolve_timeout: 5m
  
  # SMTP configuration (for email notifications)
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@mana-cicd.example.com'
  smtp_auth_username: 'alerts@mana-cicd.example.com'
  smtp_auth_password_file: '/etc/alertmanager/smtp_password'
  smtp_require_tls: true
  
  # Slack webhook URL (global, can be overridden per receiver)
  slack_api_url_file: '/etc/alertmanager/slack_webhook_url'
  
  # PagerDuty integration key
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

# Templates for notification formatting
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Main routing tree
route:
  receiver: 'default'
  group_by: ['alertname', 'namespace', 'severity']
  group_wait: 10s
  group_interval: 10s  
  repeat_interval: 12h
  
  # Child routes (more specific routing rules)
  routes:
    # Critical alerts - immediate notification via PagerDuty + Slack
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 4h
      continue: false
      
      routes:
        # Orchestrator critical - page immediately
        - match:
            component: orchestrator
          receiver: 'orchestrator-critical'
          group_wait: 0s
          repeat_interval: 2h
        
        # Application down - page immediately
        - match:
            alertname: ApplicationDown
          receiver: 'application-critical'
          group_wait: 0s
          repeat_interval: 2h
        
        # Infrastructure critical
        - match:
            component: infrastructure
          receiver: 'infrastructure-critical'
          group_wait: 30s
          repeat_interval: 6h
    
    # Warning alerts - Slack notification only
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_wait: 30s
      group_interval: 10m
      repeat_interval: 24h
      continue: false
    
    # SLO violations - special handling
    - match:
        slo: "true"
      receiver: 'slo-violations'
      group_wait: 5m
      group_interval: 30m
      repeat_interval: 6h
      continue: false
    
    # Monitoring stack alerts - separate channel
    - match:
        component: monitoring
      receiver: 'monitoring-alerts'
      group_wait: 5m
      repeat_interval: 24h
      continue: false
    
    # Deployment-specific alerts
    - match_re:
        component: (deployment|cicd|orchestrator)
      receiver: 'deployment-alerts'
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      continue: false
    
    # Test alerts (for testing notification setup)
    - match:
        alertname: Watchdog
      receiver: 'null'
      repeat_interval: 5m

# Inhibition rules (suppress alerts based on other active alerts)
inhibit_rules:
  # Inhibit warning alerts if critical alert is firing for same service
  - source_match:
      severity: critical
    target_match:
      severity: warning
    equal: ['namespace', 'app']
  
  # If orchestrator is down, don't alert on deployment failures
  - source_match:
      alertname: OrchestratorDown
    target_match_re:
      component: (deployment|cicd)
    equal: ['namespace']
  
  # If application is down, don't alert on high error rate
  - source_match:
      alertname: ApplicationDown
    target_match:
      alertname: HighErrorRate
    equal: ['namespace', 'app']
  
  # If node is not ready, don't alert on pod issues on that node
  - source_match:
      alertname: NodeNotReady
    target_match_re:
      alertname: (PodCrashLooping|HighMemoryUsage|HighCPUUsage)
    equal: ['node']

# Notification receivers
receivers:
  # Default receiver - basic webhook to orchestrator
  - name: 'default'
    webhook_configs:
      - url: 'http://orchestrator-service.orchestrator:8000/webhook/alert'
        send_resolved: true
        http_config:
          follow_redirects: true
        max_alerts: 0

  # Critical alerts - multiple channels
  - name: 'critical-alerts'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/pagerduty_key'
        description: '{{ template "pagerduty.default.description" . }}'
        severity: 'critical'
        details:
          firing: '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
          resolved: '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
          num_firing: '{{ .Alerts.Firing | len }}'
          num_resolved: '{{ .Alerts.Resolved | len }}'
        send_resolved: true
    
    slack_configs:
      - channel: '#mana-critical'
        username: 'Mana AlertManager'
        icon_emoji: ':rotating_light:'
        title: '{{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
    
    webhook_configs:
      - url: 'http://orchestrator-service.orchestrator:8000/webhook/alert'
        send_resolved: true

  # Orchestrator critical alerts
  - name: 'orchestrator-critical'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/pagerduty_key'
        description: 'URGENT: Orchestrator Issue - {{ .CommonAnnotations.summary }}'
        severity: 'critical'
        send_resolved: true
    
    slack_configs:
      - channel: '#mana-orchestrator'
        username: 'Mana AlertManager'
        icon_emoji: ':sos:'
        title: 'URGENT: Orchestrator Alert'
        text: '{{ template "slack.orchestrator.text" . }}'
        send_resolved: true
        color: 'danger'

  # Application critical alerts
  - name: 'application-critical'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/pagerduty_key'
        description: 'URGENT: Application Down - {{ .CommonAnnotations.summary }}'
        severity: 'critical'
        send_resolved: true
    
    slack_configs:
      - channel: '#mana-incidents'
        username: 'Mana AlertManager'
        icon_emoji: ':fire:'
        title: 'Application Critical Alert'
        text: '{{ template "slack.application.text" . }}'
        send_resolved: true
        color: 'danger'

  # Infrastructure critical alerts
  - name: 'infrastructure-critical'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/pagerduty_key'
        description: 'Infrastructure Issue - {{ .CommonAnnotations.summary }}'
        severity: 'error'
        send_resolved: true
    
    slack_configs:
      - channel: '#mana-infrastructure'
        username: 'Mana AlertManager'
        icon_emoji: ':warning:'
        title: 'Infrastructure Alert'
        text: '{{ template "slack.infrastructure.text" . }}'
        send_resolved: true
        color: 'danger'

  # Warning alerts - Slack only
  - name: 'warning-alerts'
    slack_configs:
      - channel: '#mana-warnings'
        username: 'Mana AlertManager'
        icon_emoji: ':warning:'
        title: '{{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        send_resolved: true
        color: 'warning'

  # SLO violation alerts
  - name: 'slo-violations'
    slack_configs:
      - channel: '#mana-slo'
        username: 'Mana AlertManager'
        icon_emoji: ':chart_with_downwards_trend:'
        title: 'SLO Violation Alert'
        text: |
          *Alert:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Runbook:* {{ .CommonAnnotations.runbook_url }}
          
          *Firing Alerts:*
          {{ range .Alerts.Firing }}
          â€¢ {{ .Labels.alertname }} - {{ .Annotations.summary }}
          {{ end }}
        send_resolved: true
        color: 'danger'
    
    email_configs:
      - to: 'sre-team@example.com'
        from: 'alerts@mana-cicd.example.com'
        subject: 'SLO Violation: {{ .CommonAnnotations.summary }}'
        html: '{{ template "email.default.html" . }}'
        send_resolved: true

  # Monitoring stack alerts
  - name: 'monitoring-alerts'
    slack_configs:
      - channel: '#mana-monitoring'
        username: 'Mana AlertManager'
        icon_emoji: ':chart_with_upwards_trend:'
        title: 'Monitoring Stack Alert'
        text: '{{ template "slack.monitoring.text" . }}'
        send_resolved: true
        color: 'warning'

  # Deployment alerts
  - name: 'deployment-alerts'
    slack_configs:
      - channel: '#mana-deployments'
        username: 'Mana AlertManager'
        icon_emoji: ':rocket:'
        title: 'Deployment Alert'
        text: '{{ template "slack.deployment.text" . }}'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}warning{{ else }}good{{ end }}'
    
    webhook_configs:
      - url: 'http://orchestrator-service.orchestrator:8000/webhook/alert'
        send_resolved: true

  # Null receiver (for suppressing certain alerts)
  - name: 'null'

# Time intervals (for muting alerts during maintenance windows)
# time_intervals:
#   - name: 'maintenance-window'
#     time_intervals:
#       - times:
#         - start_time: '02:00'
#           end_time: '04:00'
#         weekdays: ['saturday', 'sunday']

# Mute time intervals
# mute_time_intervals:
#   - maintenance-window